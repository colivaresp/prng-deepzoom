{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-random number sequences classifier\n",
    "\n",
    "The following notebook applies the language models fitted in [prng_deepzoom_lm.ipynb](prng_deepzoom_lm.ipynb) for the classification of the sequence samples generated with deep-zoom method as described in Machicao, J., & Bruno, O. M. (2017). Improving the pseudo-randomness properties of chaotic maps using deep-zoom. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(5), 53116.\n",
    "\n",
    "## Dataset\n",
    "Data consists of a training set and a test set, each with 100 sample files divided in 11 classes.\n",
    "\n",
    "Each class corresponds to sequences generated with the deep zoom method using a different value of k, respectively from k=0 to k=10\n",
    "\n",
    "Each sample file contains a sequence of one million digits produced using deep zoom with the corresponding value of k.\n",
    "\n",
    "Files are in text format, using the following naming convention: /{Train|Test}/k{k}/k{k}-parte{0-99}.ser\n",
    "\n",
    "## Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from RNNLanguageModel import *\n",
    "from utilities import *\n",
    "%matplotlib inline\n",
    "\n",
    "class MultiClassSequenceClassifier(object):\n",
    "  \n",
    "  def __init__(self,\n",
    "               models_dict = [{'k': 0, 'log_dir': '/tmp/RNNLanguageModel/k0'}, \n",
    "                              {'k': 1, 'log_dir': '/tmp/RNNLanguageModel/k1'}, \n",
    "                              {'k': 2, 'log_dir': '/tmp/RNNLanguageModel/k2'}, \n",
    "                              {'k': 3, 'log_dir': '/tmp/RNNLanguageModel/k3'}, \n",
    "                              {'k': 4, 'log_dir': '/tmp/RNNLanguageModel/k4'}, \n",
    "                              {'k': 5, 'log_dir': '/tmp/RNNLanguageModel/k5'}, \n",
    "                              {'k': 6, 'log_dir': '/tmp/RNNLanguageModel/k6'}, \n",
    "                              {'k': 9, 'log_dir': '/tmp/RNNLanguageModel/k9'}\n",
    "                             ]\n",
    "              ):\n",
    "    self.models_dict = models_dict\n",
    "    self._supported_models = [m['k'] for m in self.models_dict]\n",
    "    np.random.seed(0)\n",
    "    \n",
    "\n",
    "  def predict_proba(self, x):\n",
    "    x = np.array(x)\n",
    "    if np.ndim(x) == 1:\n",
    "      x = np.reshape(x, (1, -1))\n",
    "    assert np.ndim(x) == 2\n",
    "\n",
    "    sequence_size = x.shape[1]\n",
    "    step_size = min(sequence_size, 32)\n",
    "    num_samples = x.shape[0]\n",
    "    batch_size = min(192, sequence_size // step_size)\n",
    "    num_models = len(self._supported_models)\n",
    "    \n",
    "    log_probabilities = np.zeros((num_samples, num_models))\n",
    "      \n",
    "    for i in range(num_models):\n",
    "      model_info = self.models_dict[i]\n",
    "      model = RNNLanguageModel(step_size = step_size,\n",
    "                               batch_size = batch_size,\n",
    "                               log_dir = model_info['log_dir'])\n",
    "      \n",
    "      print('Evaluating data against model for k={}'.format(model_info['k']))\n",
    "      log_probabilities[:,i] = model.predict_log_proba(x)\n",
    "    \n",
    "    y_probabilities = softmax(log_probabilities)\n",
    "\n",
    "    return y_probabilities\n",
    "\n",
    "      \n",
    "  def predict(self, x):\n",
    "    probabilities = self.predict_proba(x)\n",
    "    indices = np.argmax(probabilities, 1)\n",
    "    predictions = [self.models_dict[i]['k'] for i in indices]\n",
    "    return (predictions, probabilities)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_testset(ks = [0, 1, 2, 3, 4, 5, 6, 9], \n",
    "                   samples_per_k = 100,\n",
    "                   sample_len = 100000\n",
    "                  ):\n",
    "  X = np.zeros((len(ks) * samples_per_k, sample_len))\n",
    "  Y = np.zeros(len(ks) * samples_per_k)\n",
    "  for i, k in enumerate(ks):\n",
    "    data = load_data(k=k, path_pattern = 'data/DatasetKlogmap/Test/k%d/k%d-parte%d.ser', indices=range(samples_per_k))\n",
    "    data = np.array(data)\n",
    "    data = data.reshape((samples_per_k, -1))\n",
    "    X[i * samples_per_k : (i + 1) * samples_per_k] = data[:,:sample_len]\n",
    "    Y[i * samples_per_k : (i + 1) * samples_per_k] = k\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the models on the classification task\n",
    "\n",
    "* Evaluation is made on models trained for $k \\in \\{0, 1, 2, 3, 4, 5, 6, 9\\}$\n",
    "* Models were trained with only the first 20 files from the Train set, with the exception of k=4, which was allowed to train with the whole train set. \n",
    "* Evaluation is made with samples from the Test set. For this evaluation, only the first 100,000 digits from the first 10 test files for each class k were fed to the models for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=0, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=1, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=2, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=3, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=4, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=5, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=6, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for k=9, indices= range(0, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = sample_from_testset(samples_per_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = MultiClassSequenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=0\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k0/checkpoints/20180129-000608-3256-3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:33<00:00, 18.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=1\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k1/checkpoints/20180129-132650-3256-3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:38<00:00, 18.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=2\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k2/checkpoints/20180201-173149-3256-3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:38<00:00, 18.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=3\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k3/checkpoints/20180131-010735-8244-8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:36<00:00, 18.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=4\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k4/checkpoints/20180131-051737-16277-16277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:37<00:00, 18.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k5/checkpoints/20180202-121506-3256-3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:41<00:00, 18.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=6\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k6/checkpoints/20180129-064628-3256-3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:39<00:00, 18.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating data against model for k=9\n",
      "INFO:tensorflow:Restoring parameters from /tmp/RNNLanguageModel/k9/checkpoints/20180129-100630-3256-3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [24:37<00:00, 18.47s/it]\n"
     ]
    }
   ],
   "source": [
    "(predictions, probabilities) = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k=0</th>\n",
       "      <th>k=1</th>\n",
       "      <th>k=2</th>\n",
       "      <th>k=3</th>\n",
       "      <th>k=4</th>\n",
       "      <th>k=5</th>\n",
       "      <th>k=6</th>\n",
       "      <th>k=9</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>True label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521270</td>\n",
       "      <td>0.017194</td>\n",
       "      <td>0.031538</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>0.091882</td>\n",
       "      <td>0.106044</td>\n",
       "      <td>0.105136</td>\n",
       "      <td>0.105117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.520485</td>\n",
       "      <td>0.017353</td>\n",
       "      <td>0.031890</td>\n",
       "      <td>0.022179</td>\n",
       "      <td>0.092153</td>\n",
       "      <td>0.105923</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.104994</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.520967</td>\n",
       "      <td>0.017330</td>\n",
       "      <td>0.031584</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>0.092007</td>\n",
       "      <td>0.106010</td>\n",
       "      <td>0.105106</td>\n",
       "      <td>0.105079</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521055</td>\n",
       "      <td>0.017325</td>\n",
       "      <td>0.031485</td>\n",
       "      <td>0.021860</td>\n",
       "      <td>0.092127</td>\n",
       "      <td>0.105986</td>\n",
       "      <td>0.105095</td>\n",
       "      <td>0.105067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.521008</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.031781</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>0.091768</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.105043</td>\n",
       "      <td>0.105018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.521337</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>0.031552</td>\n",
       "      <td>0.021749</td>\n",
       "      <td>0.092084</td>\n",
       "      <td>0.106059</td>\n",
       "      <td>0.105148</td>\n",
       "      <td>0.105132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.521113</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.031571</td>\n",
       "      <td>0.021694</td>\n",
       "      <td>0.092189</td>\n",
       "      <td>0.105985</td>\n",
       "      <td>0.105082</td>\n",
       "      <td>0.105062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.520845</td>\n",
       "      <td>0.017262</td>\n",
       "      <td>0.031844</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.092078</td>\n",
       "      <td>0.105953</td>\n",
       "      <td>0.105048</td>\n",
       "      <td>0.105015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.520879</td>\n",
       "      <td>0.017308</td>\n",
       "      <td>0.031899</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>0.091896</td>\n",
       "      <td>0.105939</td>\n",
       "      <td>0.105041</td>\n",
       "      <td>0.105017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.520754</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>0.031932</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.091830</td>\n",
       "      <td>0.105977</td>\n",
       "      <td>0.105077</td>\n",
       "      <td>0.105045</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.527088</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.022654</td>\n",
       "      <td>0.101962</td>\n",
       "      <td>0.108351</td>\n",
       "      <td>0.108066</td>\n",
       "      <td>0.108004</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.527362</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.022567</td>\n",
       "      <td>0.101778</td>\n",
       "      <td>0.108323</td>\n",
       "      <td>0.108039</td>\n",
       "      <td>0.107985</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.527641</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>0.101978</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.108006</td>\n",
       "      <td>0.107949</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.527233</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>0.108273</td>\n",
       "      <td>0.107990</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.527264</td>\n",
       "      <td>0.021263</td>\n",
       "      <td>0.023036</td>\n",
       "      <td>0.101697</td>\n",
       "      <td>0.108273</td>\n",
       "      <td>0.107985</td>\n",
       "      <td>0.107935</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.527829</td>\n",
       "      <td>0.021206</td>\n",
       "      <td>0.022322</td>\n",
       "      <td>0.101950</td>\n",
       "      <td>0.108267</td>\n",
       "      <td>0.107994</td>\n",
       "      <td>0.107932</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.527234</td>\n",
       "      <td>0.021371</td>\n",
       "      <td>0.022588</td>\n",
       "      <td>0.101979</td>\n",
       "      <td>0.108306</td>\n",
       "      <td>0.108016</td>\n",
       "      <td>0.107950</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.527720</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.022447</td>\n",
       "      <td>0.101804</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.108058</td>\n",
       "      <td>0.107996</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.527216</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.022663</td>\n",
       "      <td>0.101864</td>\n",
       "      <td>0.108305</td>\n",
       "      <td>0.108022</td>\n",
       "      <td>0.107959</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.527566</td>\n",
       "      <td>0.021226</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.101874</td>\n",
       "      <td>0.108251</td>\n",
       "      <td>0.107970</td>\n",
       "      <td>0.107907</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>0.522925</td>\n",
       "      <td>0.022187</td>\n",
       "      <td>0.108966</td>\n",
       "      <td>0.112956</td>\n",
       "      <td>0.112768</td>\n",
       "      <td>0.112693</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.523149</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>0.108884</td>\n",
       "      <td>0.112838</td>\n",
       "      <td>0.112659</td>\n",
       "      <td>0.112584</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.523499</td>\n",
       "      <td>0.022402</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.112842</td>\n",
       "      <td>0.112664</td>\n",
       "      <td>0.112597</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.522602</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>0.113024</td>\n",
       "      <td>0.112829</td>\n",
       "      <td>0.112773</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.523028</td>\n",
       "      <td>0.022402</td>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.112914</td>\n",
       "      <td>0.112717</td>\n",
       "      <td>0.112649</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>0.522743</td>\n",
       "      <td>0.022695</td>\n",
       "      <td>0.109025</td>\n",
       "      <td>0.112866</td>\n",
       "      <td>0.112690</td>\n",
       "      <td>0.112618</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.523157</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.108850</td>\n",
       "      <td>0.112904</td>\n",
       "      <td>0.112706</td>\n",
       "      <td>0.112640</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.523122</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>0.108928</td>\n",
       "      <td>0.112852</td>\n",
       "      <td>0.112669</td>\n",
       "      <td>0.112607</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.523328</td>\n",
       "      <td>0.022414</td>\n",
       "      <td>0.108907</td>\n",
       "      <td>0.112839</td>\n",
       "      <td>0.112641</td>\n",
       "      <td>0.112572</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>0.522802</td>\n",
       "      <td>0.022412</td>\n",
       "      <td>0.109167</td>\n",
       "      <td>0.112936</td>\n",
       "      <td>0.112751</td>\n",
       "      <td>0.112686</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.041797</td>\n",
       "      <td>0.042477</td>\n",
       "      <td>0.220299</td>\n",
       "      <td>0.226935</td>\n",
       "      <td>0.226910</td>\n",
       "      <td>0.226899</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.041705</td>\n",
       "      <td>0.043005</td>\n",
       "      <td>0.220536</td>\n",
       "      <td>0.226746</td>\n",
       "      <td>0.226732</td>\n",
       "      <td>0.226725</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.042346</td>\n",
       "      <td>0.220583</td>\n",
       "      <td>0.226814</td>\n",
       "      <td>0.226807</td>\n",
       "      <td>0.226777</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.011576</td>\n",
       "      <td>0.042233</td>\n",
       "      <td>0.042957</td>\n",
       "      <td>0.220320</td>\n",
       "      <td>0.226648</td>\n",
       "      <td>0.226621</td>\n",
       "      <td>0.226598</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.041929</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.220215</td>\n",
       "      <td>0.226935</td>\n",
       "      <td>0.226904</td>\n",
       "      <td>0.226877</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.041735</td>\n",
       "      <td>0.042972</td>\n",
       "      <td>0.220278</td>\n",
       "      <td>0.226901</td>\n",
       "      <td>0.226886</td>\n",
       "      <td>0.226867</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>0.042487</td>\n",
       "      <td>0.220252</td>\n",
       "      <td>0.227021</td>\n",
       "      <td>0.227006</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.042187</td>\n",
       "      <td>0.042429</td>\n",
       "      <td>0.220470</td>\n",
       "      <td>0.226824</td>\n",
       "      <td>0.226795</td>\n",
       "      <td>0.226771</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.041444</td>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.220578</td>\n",
       "      <td>0.226911</td>\n",
       "      <td>0.226897</td>\n",
       "      <td>0.226840</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.003041</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>0.042704</td>\n",
       "      <td>0.220490</td>\n",
       "      <td>0.226884</td>\n",
       "      <td>0.226869</td>\n",
       "      <td>0.226832</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.042671</td>\n",
       "      <td>0.220328</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.226716</td>\n",
       "      <td>0.226681</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.042110</td>\n",
       "      <td>0.042702</td>\n",
       "      <td>0.220403</td>\n",
       "      <td>0.226759</td>\n",
       "      <td>0.226772</td>\n",
       "      <td>0.226740</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.042395</td>\n",
       "      <td>0.220568</td>\n",
       "      <td>0.226756</td>\n",
       "      <td>0.226789</td>\n",
       "      <td>0.226790</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.042896</td>\n",
       "      <td>0.220671</td>\n",
       "      <td>0.226689</td>\n",
       "      <td>0.226693</td>\n",
       "      <td>0.226683</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.041917</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.220647</td>\n",
       "      <td>0.226914</td>\n",
       "      <td>0.226921</td>\n",
       "      <td>0.226897</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.042026</td>\n",
       "      <td>0.042455</td>\n",
       "      <td>0.220515</td>\n",
       "      <td>0.226862</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>0.226847</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>0.041548</td>\n",
       "      <td>0.042836</td>\n",
       "      <td>0.220718</td>\n",
       "      <td>0.226865</td>\n",
       "      <td>0.226865</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.042041</td>\n",
       "      <td>0.042470</td>\n",
       "      <td>0.220620</td>\n",
       "      <td>0.226849</td>\n",
       "      <td>0.226828</td>\n",
       "      <td>0.226815</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.041858</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.220574</td>\n",
       "      <td>0.226960</td>\n",
       "      <td>0.226962</td>\n",
       "      <td>0.226956</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>0.041727</td>\n",
       "      <td>0.042662</td>\n",
       "      <td>0.220643</td>\n",
       "      <td>0.226878</td>\n",
       "      <td>0.226894</td>\n",
       "      <td>0.226916</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>0.041797</td>\n",
       "      <td>0.042757</td>\n",
       "      <td>0.220419</td>\n",
       "      <td>0.226861</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.226895</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.042149</td>\n",
       "      <td>0.042569</td>\n",
       "      <td>0.220565</td>\n",
       "      <td>0.226775</td>\n",
       "      <td>0.226789</td>\n",
       "      <td>0.226807</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>0.041748</td>\n",
       "      <td>0.042851</td>\n",
       "      <td>0.220451</td>\n",
       "      <td>0.226816</td>\n",
       "      <td>0.226863</td>\n",
       "      <td>0.226850</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>0.041737</td>\n",
       "      <td>0.042633</td>\n",
       "      <td>0.220357</td>\n",
       "      <td>0.226992</td>\n",
       "      <td>0.227023</td>\n",
       "      <td>0.227036</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.011627</td>\n",
       "      <td>0.041753</td>\n",
       "      <td>0.042676</td>\n",
       "      <td>0.220439</td>\n",
       "      <td>0.226834</td>\n",
       "      <td>0.226842</td>\n",
       "      <td>0.226841</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>0.220810</td>\n",
       "      <td>0.227114</td>\n",
       "      <td>0.227164</td>\n",
       "      <td>0.227156</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.011462</td>\n",
       "      <td>0.042279</td>\n",
       "      <td>0.042672</td>\n",
       "      <td>0.219872</td>\n",
       "      <td>0.226908</td>\n",
       "      <td>0.226892</td>\n",
       "      <td>0.226903</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.011361</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>0.042504</td>\n",
       "      <td>0.220648</td>\n",
       "      <td>0.226825</td>\n",
       "      <td>0.226867</td>\n",
       "      <td>0.226866</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>0.041617</td>\n",
       "      <td>0.042649</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>0.226878</td>\n",
       "      <td>0.226901</td>\n",
       "      <td>0.226932</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.011395</td>\n",
       "      <td>0.041341</td>\n",
       "      <td>0.042665</td>\n",
       "      <td>0.220743</td>\n",
       "      <td>0.226932</td>\n",
       "      <td>0.226946</td>\n",
       "      <td>0.226962</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         k=0       k=1       k=2       k=3       k=4       k=5       k=6  \\\n",
       "0   0.521270  0.017194  0.031538  0.021819  0.091882  0.106044  0.105136   \n",
       "1   0.520485  0.017353  0.031890  0.022179  0.092153  0.105923  0.105023   \n",
       "2   0.520967  0.017330  0.031584  0.021917  0.092007  0.106010  0.105106   \n",
       "3   0.521055  0.017325  0.031485  0.021860  0.092127  0.105986  0.105095   \n",
       "4   0.521008  0.017284  0.031781  0.022133  0.091768  0.105964  0.105043   \n",
       "5   0.521337  0.016940  0.031552  0.021749  0.092084  0.106059  0.105148   \n",
       "6   0.521113  0.017304  0.031571  0.021694  0.092189  0.105985  0.105082   \n",
       "7   0.520845  0.017262  0.031844  0.021955  0.092078  0.105953  0.105048   \n",
       "8   0.520879  0.017308  0.031899  0.022020  0.091896  0.105939  0.105041   \n",
       "9   0.520754  0.017401  0.031932  0.021983  0.091830  0.105977  0.105077   \n",
       "10  0.002528  0.527088  0.021347  0.022654  0.101962  0.108351  0.108066   \n",
       "11  0.002543  0.527362  0.021404  0.022567  0.101778  0.108323  0.108039   \n",
       "12  0.002599  0.527641  0.020985  0.022542  0.101978  0.108300  0.108006   \n",
       "13  0.002544  0.527233  0.021460  0.022591  0.101990  0.108273  0.107990   \n",
       "14  0.002546  0.527264  0.021263  0.023036  0.101697  0.108273  0.107985   \n",
       "15  0.002500  0.527829  0.021206  0.022322  0.101950  0.108267  0.107994   \n",
       "16  0.002556  0.527234  0.021371  0.022588  0.101979  0.108306  0.108016   \n",
       "17  0.002545  0.527720  0.021098  0.022447  0.101804  0.108333  0.108058   \n",
       "18  0.002570  0.527216  0.021400  0.022663  0.101864  0.108305  0.108022   \n",
       "19  0.002509  0.527566  0.021226  0.022698  0.101874  0.108251  0.107970   \n",
       "20  0.001625  0.005879  0.522925  0.022187  0.108966  0.112956  0.112768   \n",
       "21  0.001609  0.005811  0.523149  0.022465  0.108884  0.112838  0.112659   \n",
       "22  0.001609  0.005711  0.523499  0.022402  0.108676  0.112842  0.112664   \n",
       "23  0.001625  0.005720  0.522602  0.022516  0.108912  0.113024  0.112829   \n",
       "24  0.001640  0.005660  0.523028  0.022402  0.108990  0.112914  0.112717   \n",
       "25  0.001616  0.005749  0.522743  0.022695  0.109025  0.112866  0.112690   \n",
       "26  0.001636  0.005743  0.523157  0.022364  0.108850  0.112904  0.112706   \n",
       "27  0.001632  0.005688  0.523122  0.022502  0.108928  0.112852  0.112669   \n",
       "28  0.001605  0.005694  0.523328  0.022414  0.108907  0.112839  0.112641   \n",
       "29  0.001617  0.005629  0.522802  0.022412  0.109167  0.112936  0.112751   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "50  0.003019  0.011665  0.041797  0.042477  0.220299  0.226935  0.226910   \n",
       "51  0.003010  0.011540  0.041705  0.043005  0.220536  0.226746  0.226732   \n",
       "52  0.002987  0.011390  0.042297  0.042346  0.220583  0.226814  0.226807   \n",
       "53  0.003047  0.011576  0.042233  0.042957  0.220320  0.226648  0.226621   \n",
       "54  0.002960  0.011550  0.041929  0.042630  0.220215  0.226935  0.226904   \n",
       "55  0.003049  0.011312  0.041735  0.042972  0.220278  0.226901  0.226886   \n",
       "56  0.003024  0.011472  0.041766  0.042487  0.220252  0.227021  0.227006   \n",
       "57  0.003022  0.011502  0.042187  0.042429  0.220470  0.226824  0.226795   \n",
       "58  0.003053  0.011520  0.041444  0.042756  0.220578  0.226911  0.226897   \n",
       "59  0.003041  0.011618  0.041561  0.042704  0.220490  0.226884  0.226869   \n",
       "60  0.003051  0.011558  0.042296  0.042671  0.220328  0.226700  0.226716   \n",
       "61  0.002990  0.011525  0.042110  0.042702  0.220403  0.226759  0.226772   \n",
       "62  0.003008  0.011501  0.042194  0.042395  0.220568  0.226756  0.226789   \n",
       "63  0.002960  0.011419  0.041990  0.042896  0.220671  0.226689  0.226693   \n",
       "64  0.003081  0.011496  0.041917  0.042127  0.220647  0.226914  0.226921   \n",
       "65  0.003050  0.011385  0.042026  0.042455  0.220515  0.226862  0.226859   \n",
       "66  0.003050  0.011261  0.041548  0.042836  0.220718  0.226865  0.226865   \n",
       "67  0.003003  0.011373  0.042041  0.042470  0.220620  0.226849  0.226828   \n",
       "68  0.002990  0.011526  0.041858  0.042174  0.220574  0.226960  0.226962   \n",
       "69  0.002984  0.011298  0.041727  0.042662  0.220643  0.226878  0.226894   \n",
       "70  0.002980  0.011390  0.041797  0.042757  0.220419  0.226861  0.226900   \n",
       "71  0.002953  0.011392  0.042149  0.042569  0.220565  0.226775  0.226789   \n",
       "72  0.002986  0.011436  0.041748  0.042851  0.220451  0.226816  0.226863   \n",
       "73  0.002954  0.011268  0.041737  0.042633  0.220357  0.226992  0.227023   \n",
       "74  0.002987  0.011627  0.041753  0.042676  0.220439  0.226834  0.226842   \n",
       "75  0.002983  0.011367  0.041073  0.042333  0.220810  0.227114  0.227164   \n",
       "76  0.003011  0.011462  0.042279  0.042672  0.219872  0.226908  0.226892   \n",
       "77  0.002945  0.011361  0.041983  0.042504  0.220648  0.226825  0.226867   \n",
       "78  0.002962  0.011559  0.041617  0.042649  0.220503  0.226878  0.226901   \n",
       "79  0.003017  0.011395  0.041341  0.042665  0.220743  0.226932  0.226946   \n",
       "\n",
       "         k=9  Prediction  True label  \n",
       "0   0.105117           0         0.0  \n",
       "1   0.104994           0         0.0  \n",
       "2   0.105079           0         0.0  \n",
       "3   0.105067           0         0.0  \n",
       "4   0.105018           0         0.0  \n",
       "5   0.105132           0         0.0  \n",
       "6   0.105062           0         0.0  \n",
       "7   0.105015           0         0.0  \n",
       "8   0.105017           0         0.0  \n",
       "9   0.105045           0         0.0  \n",
       "10  0.108004           1         1.0  \n",
       "11  0.107985           1         1.0  \n",
       "12  0.107949           1         1.0  \n",
       "13  0.107920           1         1.0  \n",
       "14  0.107935           1         1.0  \n",
       "15  0.107932           1         1.0  \n",
       "16  0.107950           1         1.0  \n",
       "17  0.107996           1         1.0  \n",
       "18  0.107959           1         1.0  \n",
       "19  0.107907           1         1.0  \n",
       "20  0.112693           2         2.0  \n",
       "21  0.112584           2         2.0  \n",
       "22  0.112597           2         2.0  \n",
       "23  0.112773           2         2.0  \n",
       "24  0.112649           2         2.0  \n",
       "25  0.112618           2         2.0  \n",
       "26  0.112640           2         2.0  \n",
       "27  0.112607           2         2.0  \n",
       "28  0.112572           2         2.0  \n",
       "29  0.112686           2         2.0  \n",
       "..       ...         ...         ...  \n",
       "50  0.226899           5         5.0  \n",
       "51  0.226725           5         5.0  \n",
       "52  0.226777           5         5.0  \n",
       "53  0.226598           5         5.0  \n",
       "54  0.226877           5         5.0  \n",
       "55  0.226867           5         5.0  \n",
       "56  0.226972           5         5.0  \n",
       "57  0.226771           5         5.0  \n",
       "58  0.226840           5         5.0  \n",
       "59  0.226832           5         5.0  \n",
       "60  0.226681           6         6.0  \n",
       "61  0.226740           6         6.0  \n",
       "62  0.226790           9         6.0  \n",
       "63  0.226683           6         6.0  \n",
       "64  0.226897           6         6.0  \n",
       "65  0.226847           5         6.0  \n",
       "66  0.226859           5         6.0  \n",
       "67  0.226815           5         6.0  \n",
       "68  0.226956           6         6.0  \n",
       "69  0.226916           9         6.0  \n",
       "70  0.226895           6         9.0  \n",
       "71  0.226807           9         9.0  \n",
       "72  0.226850           6         9.0  \n",
       "73  0.227036           9         9.0  \n",
       "74  0.226841           6         9.0  \n",
       "75  0.227156           6         9.0  \n",
       "76  0.226903           5         9.0  \n",
       "77  0.226866           6         9.0  \n",
       "78  0.226932           9         9.0  \n",
       "79  0.226962           9         9.0  \n",
       "\n",
       "[80 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_names = ['k={}'.format(k) for k in [0, 1, 2, 3, 4, 5, 6, 9]]\n",
    "results_table = pd.DataFrame(probabilities, columns=class_names)\n",
    "results_table['Prediction'] = predictions\n",
    "results_table['True label'] = Y_test\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  :  0.8625\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print ( 'Accuracy  : ', metrics.accuracy_score(Y_test, predictions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        k=0       1.00      1.00      1.00        10\n",
      "        k=1       1.00      1.00      1.00        10\n",
      "        k=2       1.00      1.00      1.00        10\n",
      "        k=3       1.00      1.00      1.00        10\n",
      "        k=4       1.00      1.00      1.00        10\n",
      "        k=5       0.71      1.00      0.83        10\n",
      "        k=6       0.50      0.50      0.50        10\n",
      "        k=9       0.67      0.40      0.50        10\n",
      "\n",
      "avg / total       0.86      0.86      0.85        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  3  5  2]\n",
      " [ 0  0  0  0  0  1  5  4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAEmCAYAAAAeIzmqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny+iK+X2yoAgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17287b1bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(Y_test, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(cnf_matrix)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary results and discussion\n",
    "\n",
    "* Model for k=6 and k=9 were not considered to have trained succesfully. They both produced a perplexity of 10.00 and accuracy of 0.100 during training, which intuitively implies that they did not reach to the point of detecting a pattern other than a uniform random distribution.\n",
    "* As a consequence, confusion matrix results for k=6 and k=9 are comparable to random guessing.\n",
    "* On the other side, for every other class that did capture a pattern, even just a couple of points above uniformly random guessing, recall gets to 100%\n",
    "* Removing k=6 and k=9 from the matrix would push accuracy to 100%\n",
    "* Preliminary results are very encouraging, especially given that the models have been trained with only a fraction of the data, and that neither architecture nor the network hyperparameters have been tuned optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
